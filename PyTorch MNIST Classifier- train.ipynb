{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1sDIvfyz74EsVX2F6yVvOus0jalwXaKfx","timestamp":1708372206245}],"authorship_tag":"ABX9TyM7zHm/ySiQ543Lg8Ok5CqI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"W8F4UGdIgDPR"},"outputs":[],"source":["import torch\n","import torchvision\n","import torchvision.datasets as dsets\n","import torchvision.transforms as transforms\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","source":["# Hyper Parameters\n","input_size = 784\n","num_epochs = 10\n","batch_size = 100\n","\n","transform = transforms.Compose(\n","         [transforms.ToTensor(),\n","         transforms.Normalize((0.1307,), (0.3015,))])"],"metadata":{"id":"oF8sxYb8gUO-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# MNIST Dataset (Images and Labels)\n","train_dataset = dsets.MNIST(root='./data',\n","                            train=True,\n","                            transform=transform,\n","                            download=True)\n","\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n","                                            batch_size=batch_size,\n","                                            shuffle=True)\n","\n"],"metadata":{"id":"50JyE-CFgVr2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def tanh(t):\n","    return torch.div(torch.exp(t) - torch.exp(-t), torch.exp(t) + torch.exp(-t))\n","\n","def tanhPrime(t):\n","    return 1 - t*t\n","\n","def softmax(x):\n","    out, ind = torch.max(x, dim=1)\n","    y = out.unsqueeze(0)\n","    y= y.T\n","    current_data = x - y\n","\n","    data_exp = torch.exp(current_data)\n","    data_sum = torch.sum(data_exp, axis=1, keepdims=True)\n","    s = torch.divide(data_exp,data_sum)\n","\n","    return s"],"metadata":{"id":"0_QCkFXTgXpi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Neural_Network:\n","    def __init__(self, input_size=784, output_size=10, hidden_size=100):\n","        # parameters\n","        self.inputSize = input_size\n","        self.outputSize = output_size\n","        self.hiddenSize = hidden_size\n","\n","        # weights\n","        self.W1 = torch.randn(self.inputSize, self.hiddenSize)\n","        self.W1= self.W1/torch.norm(self.W1)\n","        self.b1 = torch.zeros(self.hiddenSize)\n","\n","        self.W2 = torch.randn(self.hiddenSize, self.outputSize)\n","        self.b2 = torch.zeros(self.outputSize)\n","\n","    def forward(self, X):\n","      self.z1 = torch.matmul(X, self.W1) + self.b1\n","      self.h = tanh(self.z1)\n","      self.z2 = torch.matmul(self.h, self.W2) + self.b2\n","      return softmax(self.z2)\n","\n","    def CE_loss(self, y_pred, y):\n","      loss = -torch.sum(y*(torch.log(y_pred)))\n","      return loss/batch_size\n","\n","    def backward(self, X, y, y_pred, lr):\n","        dl_dz2 = (y_pred - y)/batch_size\n","\n","        dl_dh = torch.matmul(dl_dz2, torch.t(self.W2))\n","        dl_dz1 = dl_dh * tanhPrime(self.h)\n","\n","        self.W1 -= lr*torch.matmul(torch.t(X), dl_dz1)\n","        self.b1 -= lr*torch.matmul(torch.t(dl_dz1), torch.ones(batch_size))\n","        self.W2 -= lr*torch.matmul(torch.t(self.h), dl_dz2)\n","        self.b2 -= lr*torch.matmul(torch.t(dl_dz2), torch.ones(batch_size))\n","\n","\n","    def train (self):\n","      train_count =[]\n","      train_acc =[]\n","      train_total =0\n","      train_correct = 0\n","\n","      for epoch in range(num_epochs):\n","\n","            if epoch <4:\n","              lr=0.1\n","            else:\n","              lr= 0.001\n","\n","\n","\n","            for i, (images, labels) in enumerate(train_loader):\n","\n","              images= images.view(-1, 784)\n","              one_hot= torch.zeros(labels.size(0), 10)\n","              one_hot[torch.arange(labels.size(0)), labels]=1\n","\n","              #forward\n","              y_pred = nn.forward(images)\n","              _, predicted = torch.max(y_pred.data, 1)\n","\n","              train_correct += ((predicted == labels).sum())\n","              train_total += labels.size(0)\n","\n","              train_count.append(train_total)\n","\n","              #loss\n","              loss = nn.CE_loss(y_pred,one_hot)\n","\n","              #backward\n","              nn.backward(images, one_hot, y_pred, lr)\n","\n","            train_acc.append(train_correct/train_total)\n","\n","            if (i+1) % 200==0:\n","                print('Epoch [%d/%d], Iter [%d/%d] Loss: %.4f' %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data))\n","\n","      epochs_lst =list(range(1,num_epochs+1))\n","      plt.plot(epochs_lst,train_acc, color= 'purple')\n","      plt.title(\"Accuracy Value per Epochs\")\n","      plt.xlabel(\"Epoch\")\n","      plt.ylabel(\"Accuracy\")\n","      plt.legend()\n","      plt.show()"],"metadata":{"id":"PrCXdKWegZIy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nn = Neural_Network()\n","nn.train()\n","with open(\"train_hw1_206238891.pkl\", \"wb\") as f:\n","     pickle.dump(nn, f)"],"metadata":{"id":"MwvuD78oga7I"},"execution_count":null,"outputs":[]}]}